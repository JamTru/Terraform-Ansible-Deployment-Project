# COSC2759 Assignment 2

## Student details

- Full Name/Names: **Jamie Truong | Abel John**
- Student ID/IDs: **S3947728 | S3953018**

## Contents

- [Summary](#summary)
  - [Tools Used](#tools-used)
- [Deployment](#deployment)
  - [Deploying from Shell Script](#deploying-from-shell-script)
  - [GitHub Actions Workflow](#github-actions-workflow)
- [Infrastructure](#infrastructure)
  - [Terraform](#terraform)
  - [Ansible](#ansible)
- [Diagrams](#diagrams)

## Summary
The solution is designed to accomplish Alpine's business goal of automating their deployment process with greater resiliency by utilising Terraform and Ansible as the primary tools to handle the creation and configuration of the underlying infrastructure the Foo application and database will be hosted on.

The tools are designed such that they will automate the process of provisioning resources and configuring them according to specifications desired by the developer when deploying.

Terraform is responsible for the creation of virtual private cloud along with necessary resources on AWS. It will read a set of configurations pre-written for it, then automatically handle the planning and execution of creating resources based on dependencies without needing human input, thus preventing human error in either the specific configurations of each individual resource or the order of creation of resources.

Ansible is responsible for the configuration of the virtual private cloud resources, by running a defined order of commands to execute according to a `playbook` file. This is done in conjunction with Docker, which containerises the application into an individual image that handles its own dependencies and makes the application system-agnostic in terms of deployment. This thus automates the need to SSH into the individual containers and run all the individual commands, making it far more efficient and eliminates potential for human error.

Furthermore, the only inputs for the solution come from needing the individual deploying the script to confirm changes, such as accepting the initial resource changes from Terraform and confirming the usage of private keys in the EC2 instances. Otherwise, the deployment script automates the need for copy pasting over variables such as IP addresses, docker images, directory locations etc. which eliminates another potential area for human error from the deployment process.

### Tools Used
- GitHub (GitHub Org)
- GitHub Actions â€“ used for creating the pipelines
- Terraform
- Ansible
- AWS
- Docker

## Deployment

#### Deploying from Shell Script

To deploy using the `deploy.sh` shell script, run the following at the root directory of the repository:
```bash
~s3947728-s3953018-assignment-2# bash deploy.sh
```

The deployment shell script will handle the following tasks in this sequence:

1. Validate the User's AWS Credentials and ensure they exist.
    > Note: This does not ensure that the `.aws/credentials` will match the cloud that the EC2 instances deploy in. Please ensure that credentials are updated.
2. Ensure public/private key pair exists within the `./terraform-infra` directory.
    > Note: If the script does not detect that a `local_pub_key` exists within the directory, it will generate one for you. This is hidden by the `.gitignore` file for security reasons.
3. Initialise, validate and execute Terraform files.
    > Note: Refer to Terraform section for in-depth explanation of `deployment.tf`.
4. Save the output of Terraform as variables, as well as writing it to a `.ini` file.
    > Note: At this stage, the EC2 instances' public IP address and DNS are saved. They are then also written into the `inventory.ini` to be used as inventory files for Ansible. Additionally, the `"inventory.ini"` file is renamed to 'inventory.ini' file for formatting reasons. It is not quite understood why it is initially outputted as `"inventory.ini"` by Terraform, but the step is necessary so Ansible can read it.
5. Initialise the Database Container using Terraform outputs through Ansible.
    > Note: Refer to Ansible section for in-depth explanation of Ansible playbook structure. Ansible command takes input of `inventory.ini` file to have access to EC2 instance IP addresses, and reuses the same private key defined earlier within bash script.
6. Initialise the Application Container using Terraform ouputs through Ansible.
    > Note: Refer to Ansible Section for in-depth explanation of Ansible playbook structure. To avoid any possible issues with dependencies, the application is initialised after the database is initialised to avoid any problems with the application needing to make a connection before a database exists.
7.  Echo HTTP Link to Application
    > Note: This is purely for debugging purposes as it provides an easy method of accessing application from terminal.

![Deployment Bash Script Processes](misc/process_diagram.svg)

#### GitHub Actions Workflow


## Infrastructure
### Terraform
![AWS Infrastructure Generated by Terraform](https://github.com/user-attachments/assets/1f515390-07d4-44a8-887b-144234f1adf9)

> VPC and Subnets
- Creates a VPC with CIDR block 10.0.0.0/16
- Defines two public subnets (10.0.1.0/24 and 10.0.2.0/24) across two Availability Zones (us-east-1a and us-east-1b) for deploying the application instances.
- Has been configured with `enable_dns_support = true` and `enable_dns_hostnames = true` to allow for the EC2 instances to have public DNS names.
- The subnets contain the setting `map_public_ip_on_launch = true` to ensure that EC2 instances deployed inside the subnet will always have a public IP address to reference.

> Security Groups
- 2 Security groups are created for both app and database instances
Both security groups are open on:
- Port 22: For Ansible to be able to SSH in to configure the instances
- Port 80: To allow users to access the application through the IP address provided.
- Port 5432: To allow for the instances to communicate to each other and access / transfer information from the database instance.

> Application Instances
- 2 Application Instances using `t.2 micro` instance type launched in two different public subnets
- `associate_public_ip_address = true` setting is made explicit to ensure that the EC2 has a public IP address associated with it within the VPC

> Load Balancer
- Distribute traffic across the two application instances

> Database Instance
- Creates a single database instance using the same `t2.micro instance` type with the same AMI as the application instances
- Launches the database instance in a public subnet

### Ansible

The Ansible aspect of the solution is designed to automate the configuration of the EC2 instances by remotely connecting into the EC2 instances set up and outputted by Terraform via SSH. To accomplish this, an INI file is generated by Terraform that is split into the following:
```
[app]
app1 ansible_host=${app1_dns} app_ip=${app1_ip}
app2 ansible_host=${app2_dns} app_ip=${app2_ip}

[database]
db1 ansible_host=${db_dns} db_ip=${db_ip}
```
where `[app]` represents the number of app instances generated by Terraform, whereas `[database]` represents the number of database instances generated by Terraform. The decision to use INI files over YAML Playbooks was due to the ability to customise and insert variables into INI files that was absent from the playbooks. Thus YAML playbooks was incompatible with the automation due to the constant changing of IP addresses when generating new EC2 instances.

> Playbook Breakdown

The steps of the Application Playbook is as follows:
1. Retrieve and Install Docker
2. Pull Docker Image
3. Create Docker Container
The playbook was written so that the Docker Image is a variable passed through as a command to allow for flexibility, allowing any future modifications to simply change the docker image needed to be passed through. As well, to allow the application instance to be granted database access, it is configured the ENV variables with the database instance IP address and the login/password of database user.

For the database instance, the Database Playbook follows these steps:
1. Retrieve and Install Docker
2. Pull Postgres Docker Image
3. Copy SQL file from local to instance
4. Create Postgres Container with data mounted

As it is unlikely for the database to change as often as the application's docker image, it was decided to hard-code the Postgres image into the playbook for convenience. Additionally, it was necessary to directly copy the SQL file into the EC2 instance as it then allowed for the mounting of the SQL as a volume and thus set up the database on deployment before any applications need to access it.

Additionally, Ansible by default searches for files to copy from the directory it is executed in, within the subdirectory `/files`, and as such, it was easier to copy over the SQL into the `ansible/files` directory to make it easier to retrieve.
